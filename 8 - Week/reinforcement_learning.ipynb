{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "In this notebook you will learn how to implement from scratch the Q-Learning algorithm for deterministic Markov\n",
    "Decision Process and how to use the Deep Q-Learning algorithm using the TensorFlow-Agents library.\n",
    "\n",
    "Imagine we want to train an impostor from\n",
    "[Among Us](https://en.wikipedia.org/wiki/Among_Us) to make it capable of finding the closest vent to\n",
    "hide himself. To simplify this task we assume that the impostor moves in a grid world.\n",
    "\n",
    "We can model the impostor in this world as an agent that implements 4 actions,\n",
    "move up, move down, move right and move left,\n",
    "and we can model the environment as a grid world where the agent gets rewarded only once it has achieved its goal,\n",
    "it has found the vent.\n",
    "\n",
    "## The Environment\n",
    "\n",
    "If in supervised learning, we would usually spend most of our time collecting and cleaning the data, in RL this\n",
    "time is spent in designing the environment. Following we design a `GridWorld` class."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "\n",
    "class GridWold:\n",
    "\n",
    "    def __init__(self, n, init_pos=(0, 0), target_pos=None):\n",
    "        \"\"\"\n",
    "        A simple environment\n",
    "        :param n: The size of the world.\n",
    "        :param init_pos: The initial position of the agent. The agent start at the top-left corner by default.\n",
    "        :param target_pos: The position of the goal. The goals is set to the bottom-right corner by default.\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.init_pos = init_pos\n",
    "        self.pos = init_pos\n",
    "        if target_pos is None:\n",
    "            self.target_pos = (n - 1, n - 1)\n",
    "        else:\n",
    "            self.target_pos = target_pos\n",
    "\n",
    "    def reset(self, pos=None):\n",
    "        \"\"\"\n",
    "        This method resets the environment.\n",
    "        :param pos: If we set the position, the agent will start not from the default position by from the\n",
    "        position set. This is useful when we will add some randomness in the training.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if pos is None:\n",
    "            self.pos = self.init_pos\n",
    "        else:\n",
    "            self.pos = pos\n",
    "        return self.pos\n",
    "\n",
    "    def get_available_actions(self):\n",
    "        \"\"\"\n",
    "        This method returns the set of available actions the agent can perform given its current position.\n",
    "        :return: The set of available actions.\n",
    "        \"\"\"\n",
    "        # if the agent has reached the vent, then no actions are available.\n",
    "        if self.pos == self.target_pos:\n",
    "            res = {}\n",
    "        else:\n",
    "            # all actions\n",
    "            res = {'up', 'down', 'right', 'left'}\n",
    "            # remove actions if the agent is at the edges of the world.\n",
    "            if self.pos[0] == 0:\n",
    "                res.discard('left')\n",
    "            elif self.pos[0] == self.n - 1:\n",
    "                res.discard('right')\n",
    "            if self.pos[1] == 0:\n",
    "                res.discard('up')\n",
    "            elif self.pos[1] == self.n - 1:\n",
    "                res.discard('down')\n",
    "        return res\n",
    "\n",
    "    def step(self, action: str):\n",
    "        \"\"\"\n",
    "        This method executes an action and changes the state of the agent. It returns the new position of the agent and\n",
    "        the reward received.\n",
    "        :param action: The action to execute.\n",
    "        :return: The position and reward.\n",
    "        \"\"\"\n",
    "        # executes an action\n",
    "        if action == 'left':\n",
    "            self.pos = (self.pos[0] - 1, self.pos[1])\n",
    "        elif action == 'right':\n",
    "            self.pos = (self.pos[0] + 1, self.pos[1])\n",
    "        elif action == 'down':\n",
    "            self.pos = (self.pos[0], self.pos[1] + 1)\n",
    "        elif action == 'up':\n",
    "            self.pos = (self.pos[0], self.pos[1] - 1)\n",
    "\n",
    "        # if the agent has achieved the goal, it returns a reward of 100, zero otherwise.\n",
    "        if self.pos == self.target_pos:\n",
    "            return self.pos, 100\n",
    "        else:\n",
    "            return self.pos, 0\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        This method renders the environment and the agent.\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        agent = OffsetImage(plt.imread('imgs/agent.png'), zoom=1 / (2 * n))\n",
    "        target = OffsetImage(plt.imread('imgs/target.png'), zoom=10 / (2 * n))\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        for j in range(n + 1):\n",
    "            ax.axvline(x=j / n)\n",
    "            ax.axhline(y=j / n)\n",
    "\n",
    "        ab_target = AnnotationBbox(target,\n",
    "                                   xy=((self.target_pos[0] + 0.5) / n, 1 - (self.target_pos[1] + 0.7) / n),\n",
    "                                   frameon=False)\n",
    "        ax.add_artist(ab_target)\n",
    "\n",
    "        ab_agent = AnnotationBbox(agent,\n",
    "                                  xy=((self.pos[0] + 0.5) / n, 1 - (self.pos[1] + 0.5) / n),\n",
    "                                  frameon=False)\n",
    "        ax.add_artist(ab_agent)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now instantiate a gird-world of size 5 and render it. If we use the default parameters, you should see the\n",
    "impostor on the top-left corner, and the vent on the bottom-right corner."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-117-f1e48fcdb2af>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mavailable_actions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpos\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m     \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrender\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreward\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpos\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-113-2a8994c82ed2>\u001B[0m in \u001B[0;36mrender\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     57\u001B[0m         \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_artist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mab_target\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m         \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 59\u001B[0;31m         \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = GridWold(5)\n",
    "env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Random Policy\n",
    "\n",
    "We will now simulate a random policy. An agent that navigates the world performing random actions. This agent will\n",
    "execute actions for a given number of times. While executing the actions\n",
    "we will also count the number of times the agent has achieved its goal (the number of episodes).\n",
    "\n",
    "Note that we are slowing down the execution of this policy to make this policy visible; We add a 1ms delay at every\n",
    "agent step."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "\n",
    "# the number of actions the agent will execute in total\n",
    "total_steps = 1000\n",
    "\n",
    "episodes = 0\n",
    "for n in tqdm(range(total_steps)):\n",
    "    # delay\n",
    "    time.sleep(0.1)\n",
    "    display.clear_output(wait=True)\n",
    "    available_actions = env.get_available_actions()\n",
    "    # if no actions are available then the agent has reached the goal\n",
    "    if not available_actions:\n",
    "        env.reset()\n",
    "        episodes += 1\n",
    "    else:\n",
    "        action = random.sample(available_actions, 1)[0]\n",
    "        pos, reward = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "    print(f'Iteration: {n+1} of Episodes {episodes}')\n",
    "    print(f'Available actions: {available_actions}')\n",
    "    print(f'Action taken: {action}')\n",
    "    print(f'Position: {pos}')\n",
    "    print(f'Reward: {reward}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q-Learning\n",
    "\n",
    "We will now implement the Q-learning algorithm to learn the Q-values. These Q-values will be stored in a table called\n",
    "`q_table`. To pick the next action the agent will choose the action that maximizes the Q-value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def key_with_max_value(d):\n",
    "    max_value = max(d.values())\n",
    "    max_keys = []\n",
    "    for key, value in d.items():\n",
    "        if value == max_value:\n",
    "            max_keys.append(key)\n",
    "    return random.sample(max_keys, 1)[0]\n",
    "\n",
    "# learning rate\n",
    "l = 0.9\n",
    "# the number of actions the agent will execute in total\n",
    "total_steps = 1000\n",
    "# q-values\n",
    "q_table = {}\n",
    "\n",
    "# reset env\n",
    "pos = env.reset()\n",
    "\n",
    "# render env\n",
    "env.render()\n",
    "time.sleep(0.1)\n",
    "display.clear_output(wait=True)\n",
    "\n",
    "episodes = 0\n",
    "for n in tqdm(range(total_steps)):\n",
    "    # delay\n",
    "    time.sleep(0.1)\n",
    "    display.clear_output(wait=True)\n",
    "    # get available actions\n",
    "    available_actions = env.get_available_actions()\n",
    "    # if no actions are available then the agent has reached the goal\n",
    "    if not available_actions:\n",
    "        episodes += 1\n",
    "        env.reset()\n",
    "    else:\n",
    "        # if this state exists in the q_table extract the actions\n",
    "        if pos in q_table:\n",
    "            action_probs = q_table[pos]\n",
    "        else:  # else initialize the probs to 0\n",
    "            action_probs = {a: 0 for a in available_actions}\n",
    "            q_table[pos] = action_probs\n",
    "\n",
    "        # select the best action\n",
    "        action = key_with_max_value(action_probs)\n",
    "        # execute this action\n",
    "        pos, reward = env.step(action)\n",
    "        # update the q-value\n",
    "        if pos in q_table:\n",
    "            action_probs[action] = reward + l * max(q_table[pos].values())\n",
    "        else:\n",
    "            action_probs[action] = reward\n",
    "\n",
    "    # print statistics\n",
    "    env.render()\n",
    "    print(f'Iteration: {n + 1} of Episodes {episodes}')\n",
    "    print(f'Available actions: {available_actions}')\n",
    "    print(f'Action taken: {action}')\n",
    "    print(f'Reward: {reward}')\n",
    "    print(f'Position: {pos}')\n",
    "    print(f'Q-Values:')\n",
    "    for key in sorted(q_table):\n",
    "        print(key, q_table[key])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have trained the agent we can test it. Compare the performance of this agent against the random policy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the number of actions the agent will execute in total\n",
    "total_steps = 1000\n",
    "\n",
    "# reset env\n",
    "pos = env.reset()\n",
    "for n in tqdm(range(total_steps)):\n",
    "    time.sleep(0.1)\n",
    "    display.clear_output(wait=True)\n",
    "    # get available actions\n",
    "    available_actions = env.get_available_actions()\n",
    "    if not available_actions:\n",
    "        episodes += 1\n",
    "        env.reset()\n",
    "    else:\n",
    "        # select the best action\n",
    "        action = key_with_max_value(q_table[pos])\n",
    "        # execute this action\n",
    "        pos, reward = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "    print(f'Iteration: {n + 1} of Episodes {episodes}')\n",
    "    print(f'Available actions: {available_actions}')\n",
    "    print(f'Action taken: {action}')\n",
    "    print(f'Reward: {reward}')\n",
    "    print(f'Position: {pos}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## An example of Ready-Made Environments (MiniGrid)\n",
    "\n",
    "Here I provide you with an example of a ready-made environment where you can test RL algorithms.\n",
    "These environments are defined by the [MiniGrid](https://github.com/maximecb/gym-minigrid).\n",
    "\n",
    "Before executing the next code, please make sure that this package is correctly installed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install gym-minigrid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gym_minigrid.wrappers import *\n",
    "\n",
    "env = gym.make('MiniGrid-Empty-16x16-v0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is how the environment looks like at the beginning. We have a grid world with an agent in the top-left corner\n",
    "that can navigate this empty room. On the bottom-right corner we have the target location."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can access the actions available to the agent as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for action in list(env.actions):\n",
    "    print(action)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_steps = 100\n",
    "\n",
    "for n in tqdm(range(total_steps)):\n",
    "    time.sleep(0.1)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, _, _ = env.step(action)\n",
    "\n",
    "    print(f'Iteration: {n + 1} of Episodes {episodes}')\n",
    "    print(f'Action taken: {action}')\n",
    "    print(f'Reward: {reward}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deep Q-Learning with TF-Agents\n",
    "\n",
    "This example shows how to train a\n",
    "[DQN (Deep Q Networks)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "agent using the TF-Agents library.\n",
    "\n",
    "Before executing the next code, please make sure that these packages are correctly installed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install tf-agents\n",
    "!pip install pyglet==1.5.11"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Cartpole Environment\n",
    "\n",
    "We will train a DQN agent for the Cartpole environment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tf_agents.environments import suite_gym, tf_py_environment\n",
    "\n",
    "env = suite_gym.load('CartPole-v1')\n",
    "env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "print('Observation Spec:', env.time_step_spec().observation)\n",
    "print('Reward Spec:', env.time_step_spec().reward)\n",
    "print('Action Spec:', env.action_spec())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now try executing always one action."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "time_step = env.reset()\n",
    "print('Time step:')\n",
    "print(time_step)\n",
    "\n",
    "while not time_step.is_last():\n",
    "    action = np.array(1, dtype=np.int32)\n",
    "    time_step = env.step(action)\n",
    "    env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that by doing this the episode ends quickly. This naive agent fails quickly.\n",
    "\n",
    "### The DQN Agent\n",
    "\n",
    "The DQN agent can be used in any environment which has a discrete action space.\n",
    "At the heart of a DQN Agent is a QNetwork, a neural network model that can learn to predict\n",
    "Q-Values (expected returns) for all actions, given an observation from the environment.\n",
    "\n",
    "The QNetwork consists of a sequence of Dense layers, where the final layer\n",
    "will have 1 output for each possible action."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "    return tf.keras.layers.Dense(\n",
    "        num_units,\n",
    "        activation=tf.keras.activations.relu,\n",
    "        kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "            scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer with `num_actions` units to generate one\n",
    "# q_value per available action as it's output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we instantiate a `DqnAgent`. The agent constructor also requires an\n",
    "optimizer (in this case, `AdamOptimizer`), a loss function, and an integer step counter."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss)\n",
    "\n",
    "agent.initialize()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before training the DQN agent we can test how a random policy would behave in this environment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(), env.action_spec())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also create a helper function that will allow us render any policy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def render_policy(policy):\n",
    "    time_step = env.reset()\n",
    "    env.render()\n",
    "    while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = env.step(action_step.action)\n",
    "        env.render()\n",
    "\n",
    "render_policy(random_policy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given a policy we are interested in evaluating what is its average return.\n",
    "To do that we run the policy for multiple episodes and average their returns."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "  res = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    res += episode_return\n",
    "\n",
    "  avg_return = res / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "print('Average return of the random policy:', compute_avg_return(env, random_policy))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Replay Buffer\n",
    "\n",
    "The replay buffer keeps track of data collected from the environment.\n",
    "This allows the agent to train on previous experiences. This helps in stabilizing the training.\n",
    "We use the `TFUniformReplayBuffer`, as it is the most common."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer_max_length = 100000\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now execute the random policy in the environment for a few steps, and\n",
    "record the data in the replay buffer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "initial_collect_steps = 100\n",
    "collect_data(env, random_policy, replay_buffer, initial_collect_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the DQN Agent\n",
    "\n",
    "Two things must happen during the training loop:\n",
    "-   collect data from the environment\n",
    "-   use that data to train the agent's neural network(s)\n",
    "\n",
    "This example also periodically evaluates the policy and prints the current score."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(env, agent.policy)\n",
    "returns = [avg_return]\n",
    "\n",
    "collect_steps_per_iteration = 1\n",
    "num_iterations = 10000\n",
    "iterator = iter(dataset)\n",
    "agent.train_step_counter.assign(0)\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, _ = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % 200 == 0:\n",
    "    print('step = {0}:\\n\\tloss = {1}'.format(step, train_loss))\n",
    "    avg_return = compute_avg_return(env, agent.policy)\n",
    "    print('\\taverage return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now see how the training progressed by plotting the average return over the iterations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "iterations = range(0, num_iterations + 1, 200)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now see how the agent behaves."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "render_policy(agent.policy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}