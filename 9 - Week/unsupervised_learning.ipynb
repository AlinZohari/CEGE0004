{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "In this notebook you will get familiar with the most popular dimensionality reduction and clustering algorithms.\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction methods are used to reduce the dimensionality of a dataset. These are commonly used\n",
    "for data exploration or to fight the curse of dimensionality when training supervised learning methods.\n",
    "\n",
    "In order to be able to see how these algorithms behave, we will look at some examples of data living in a 3D\n",
    "space that we would like to reduce to a 2D space.\n",
    "\n",
    "The first dimensionality reduction method that we will look at is a projection method,\n",
    "the Principal Component Analysis (PCA), then we will look at a manifold method, the Locally Linear Embeddings (LL).\n",
    "\n",
    "### PCA\n",
    "\n",
    "PCA seeks to find a hyperplane such that when projecting the original dataset onto it explains its largest variability.\n",
    "In order to see PCA in action, we will generate a very simple dataset.\n",
    "Let's now build a cloud of data points with the shape of an open ring in a 3D space."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# number of instances\n",
    "m = 500\n",
    "\n",
    "# noise terms\n",
    "ns = lambda: 0.05 * np.random.randn(m)\n",
    "\n",
    "# if you wish to close the ring you should remove the last term (15/16).\n",
    "angles = np.random.rand(m) * 2 * np.pi * 15 / 16\n",
    "xs = np.empty((m, 3))\n",
    "\n",
    "xs[:, 0] = np.cos(angles) + np.sin(angles) + ns()\n",
    "xs[:, 1] = np.sin(angles) * 0.7 + ns()\n",
    "xs[:, 2] = xs[:, 0] * 0.1 + xs[:, 1] * 0.3 + ns()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now plot the dataset that we have just generated."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_3d(xs, c=None):\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(xs[:,0], xs[:,1], xs[:,2], c=c)\n",
    "    plt.show()\n",
    "\n",
    "plot_3d(xs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This dataset has been generated with a dimensionality that can be visualized. However, imagine what we could have done\n",
    "if this was not the case. We could have plotted each dimension against the other as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.pairplot(pd.DataFrame(xs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given the simplicity of the dataset, in this plots we can infer already a lot about the content of this dataset:\n",
    "The shape lies on a 2D dimensional plane because out of the 3 plots only one has a shape that does not resemble a line.\n",
    "However, for a dataset with a larger number of features or with more complicated feature interactions this strategy\n",
    "would have not led us to anything useful. In these cases PCA may help us understand the dataset better.\n",
    "\n",
    "We will use the scikit-learn implementation of PCA. This takes as a parameter the number of components, which is the\n",
    "dimensionality of the hyperplane where the PCA will project the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "xs_2d = pca.fit_transform(xs)\n",
    "\n",
    "print('The first 5 projected points:')\n",
    "xs_2d[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now plot the result of this projection in a 2D plot:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_2d(xs, c=None):\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(xs[:, 0], xs[:, 1], c=c)\n",
    "    plt.show()\n",
    "\n",
    "plot_2d(xs_2d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, PCA has successfully projected the 3D shape into a 2D space without loosing much information about\n",
    "the original shape."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Locally Linear Embeddings\n",
    "\n",
    "Locally Linear Embeddings (LL) is a manifold based method. Manifold based methods assume that the data points do not\n",
    "come from the whole space but from a manifold.\n",
    "\n",
    "To test this algorithm we will generate our points using a helper function provided by scikit-learn. We will\n",
    "generate points with a shape of a swiss roll. Our goal is to unfold the swiss roll using LL.\n",
    "In this example we will also have a color indicator to better see which data points should be considered closer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "xs, ys = make_swiss_roll(n_samples=10000, noise=0.2)\n",
    "\n",
    "plot_3d(xs, ys)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we were to use a linear project, we would not be able to unfold the swiss roll.\n",
    "Following an example of such a projection using two dimensions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.scatter(xs[:, 0], xs[:, 1], c=ys)\n",
    "plt.grid(True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now use the LL implementation of scikit-learn. LL takes as parameters, the number of components,\n",
    "which is similar to PCA, and the number of neighbors LL should consider in order to reconstruct the manifold."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=50)\n",
    "xs_2d = lle.fit_transform(xs)\n",
    "\n",
    "plot_2d(xs_2d, ys)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here you can see that data points that have a similar color are closer to each other, meaning that the swiss roll has\n",
    "been properly unfolded.\n",
    "\n",
    "## Clustering\n",
    "\n",
    "Clustering algorithms are used to find groups in data. In this notebook we will review some of the most popular\n",
    "algorithms. We will start with K-Means, then DBSCAN, and conclude with Gaussian Mixture models.\n",
    "\n",
    "### K-Means\n",
    "\n",
    "K-means is a simple and effective algorithm. In order to test this algorithm we will generate a dataset made of blobs.\n",
    "\n",
    "The dataset that we will generate here is made of blobs with different density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# the centroids of the blobs\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "\n",
    "# the density of each blobs\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "\n",
    "xs, ys = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std)\n",
    "\n",
    "plot_2d(xs, ys)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A parameter of K-means is the number of clusters we are expecting to find in the dataset (the $k$). In this case, since\n",
    "we have constructed the dataset ourselves, we know that the best $k$ is 5. However, normally you do not know how many\n",
    "clusters to expect, and you will need to inspect your results in order to see if the result of the clustering satisfies\n",
    "you."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 5\n",
    "k_means = KMeans(n_clusters=k)\n",
    "k_means.fit(xs)\n",
    "\n",
    "print('Centroids of the clustering algorithm:')\n",
    "k_means.cluster_centers_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the K-means has successfully found all the clusters, then these centroids will be equal to the centroids we set when\n",
    "generating the dataset.\n",
    "\n",
    "To inspect this visually let's now plot the dataset with the centroids."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.scatter(xs[:, 0], xs[:, 1], c=ys)\n",
    "plt.scatter(k_means.cluster_centers_[:, 0], k_means.cluster_centers_[:, 1], c='red')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we have found the clusters, we can use k-means to predict the cluster of any new example using the predict method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we define a set of 5 points\n",
    "xs_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "\n",
    "# it will predict to which cluster these points belong\n",
    "k_means.predict(xs_new)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DBSCAN\n",
    "\n",
    "DBSCAN stands for Density-based spatial clustering of applications with noise (DBSCAN). This algorithm, given a set of\n",
    "points in some space, groups together points that are closely packed together (points with many nearby neighbors),\n",
    "marking as outliers those that lie alone in low-density regions (whose nearest neighbors are too far away).\n",
    "\n",
    "To test this clustering algorithm we will generate a dataset made of moon shapes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "xs, ys = make_moons(n_samples=500, noise=0.05)\n",
    "\n",
    "plot_2d(xs, ys)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DBSCAN takes two parameters: The $\\varepsilon$ distance which is used to decide at which distance a point should be\n",
    "considered a neighbor, and the number of samples: the number of points that within the $\\varepsilon$ distance\n",
    "qualify a point as being a core-instance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
    "dbscan.fit(xs)\n",
    "\n",
    "print('The labels provided to the first 5 points of the dataset')\n",
    "dbscan.labels_[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Among the $m$ points in the dataset DBSCAN has found the following number of core instances:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(dbscan.core_sample_indices_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DBSCAN will also tell us how many clusters it has found:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(set(dbscan.labels_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now plot each cluster with a different color and also\n",
    "those points that have been marked as anomalies."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_dbscan(model, xs):\n",
    "    core_mask = np.zeros_like(model.labels_, dtype=bool)\n",
    "    core_mask[model.core_sample_indices_] = True\n",
    "    anomalies_mask = model.labels_ == -1\n",
    "    non_core_mask = ~(core_mask | anomalies_mask)\n",
    "\n",
    "    cores = model.components_\n",
    "    anomalies = xs[anomalies_mask]\n",
    "    non_cores = xs[non_core_mask]\n",
    "\n",
    "    plt.scatter(cores[:, 0], cores[:, 1], c=model.labels_[core_mask], marker='o', s=1, cmap='Paired')\n",
    "    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=1, c=model.labels_[core_mask])\n",
    "    plt.scatter(anomalies[:, 0], anomalies[:, 1], c='r', marker='x', s=20)\n",
    "    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=model.labels_[non_core_mask], marker=\".\")\n",
    "\n",
    "plot_dbscan(dbscan, xs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given we know how many clusters are in the dataset, let's try to improve this clustering result by increasing the\n",
    "$\\varepsilon$ parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "dbscan.fit(xs)\n",
    "\n",
    "plot_dbscan(dbscan, xs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gaussian Mixture Model\n",
    "\n",
    "A Gaussian mixture model is a probabilistic model that assumes that all the data points have been generated from a\n",
    "mixture of a finite number of Gaussian distributions with unknown parameters.\n",
    "\n",
    "For this model we will use the IRIS dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "xs = data.data\n",
    "ys = data.target\n",
    "\n",
    "data.target_names\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now visualize this dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(9, 3.5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(xs[ys==0, 2], xs[ys==0, 3], \"yo\", label=\"Iris setosa\")\n",
    "plt.plot(xs[ys==1, 2], xs[ys==1, 3], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(xs[ys==2, 2], xs[ys==2, 3], \"g^\", label=\"Iris virginica\")\n",
    "plt.xlabel(\"petal length\", fontsize=14)\n",
    "plt.ylabel(\"petal width\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(xs[:, 2], xs[:, 3], c=\"k\", marker=\".\")\n",
    "plt.xlabel(\"petal length\", fontsize=14)\n",
    "plt.tick_params(labelleft=False)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the left-hand side we see the dataset with the known classification and on the right-hand side we have the dataset\n",
    "we will use to cluster them. From now on, we will assume that those classifications are unknown.\n",
    "\n",
    "Similar to k-means, GMM requires the specification of the number of clusters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=3).fit(xs)\n",
    "\n",
    "ys_pred = gmm.predict(xs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now visualize the result of this clustering."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(xs[ys_pred==0, 2], xs[ys_pred==0, 3], \"yo\", label=\"cluster 1\")\n",
    "plt.plot(xs[ys_pred==1, 2], xs[ys_pred==1, 3], \"bs\", label=\"cluster 2\")\n",
    "plt.plot(xs[ys_pred==2, 2], xs[ys_pred==2, 3], \"g^\", label=\"cluster 3\")\n",
    "plt.xlabel(\"petal length\", fontsize=14)\n",
    "plt.ylabel(\"petal width\", fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=12)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is now up to us to give the appropriate name to these clusters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}